## NLP research for data science
- Data science is a huge domain and it is easy to get lost in the domain while exploring!
- This repo is to keep track the things explored in the journey of Natural Language Processing (NLP), Natural Language Understanding (NLU), etc...

---

## NLP Research 

| Model Name                  | Research Link                                                                          |
|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| RNN seq2seq and seq2seq with attention                                          |  [link](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)    |
| Attention Is All You Need                                          |  [Official Paper](https://arxiv.org/abs/1706.03762), [illustrated-transformer](https://jalammar.github.io/illustrated-transformer), [github huggingface](https://github.com/huggingface/transformers),    |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding                                           | [Official Paper](https://arxiv.org/abs/1810.04805), [Google](https://github.com/google-research/bert), [illustrated-bert](https://jalammar.github.io/illustrated-bert)   
| GPT2: Language Models are Unsupervised Multitask Learners                                           | [Offical Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [github openai](https://github.com/openai/gpt-2), [better-language-models](https://openai.com/blog/better-language-models) ,  [illustrated-gpt2](https://jalammar.github.io/illustrated-gpt2)    |
| GPT3: Language Models are Few-Shot Learners                                           | [Official Paper](https://arxiv.org/pdf/2005.14165.pdf), [springboard gpt3](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai), [gpt3 code explain](https://simonwillison.net/2022/Jul/9/gpt-3-explain-code)  , [gpt3 viz](https://jalammar.github.io/how-gpt3-works-visualizations-animations)   |
| RoBERTa: A Robustly Optimized BERT Pretraining Approach                                           | [Official Paper](https://arxiv.org/abs/1907.11692)|
| DeBERTa: Decoding-enhanced BERT with Disentangled Attention                                           | [Official Paper](https://arxiv.org/abs/2006.03654), [microsoft blog](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark), [huggingface](https://huggingface.co/docs/transformers/model_doc/deberta)|
| DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing                                           | [Official Paper](https://arxiv.org/abs/2111.09543)|

---

## NLP General Concept's

| Concept                  | Link                                                                          |
|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| word2vec                                          |  [illustrated-word2vec](https://jalammar.github.io/illustrated-word2vec)   |
